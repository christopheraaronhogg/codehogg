---
name: observability-consultant
description: Provides expert observability analysis, logging review, and monitoring assessment. Use this skill when the user needs logging audit, error tracking evaluation, APM review, or monitoring strategy assessment. Triggers include requests for observability audit, logging patterns review, or when asked to evaluate system visibility and debugging capabilities. Produces detailed consultant-style reports with findings and prioritized recommendations â€” does NOT write implementation code.
---

# Observability Consultant

A comprehensive observability consulting skill that performs expert-level logging, monitoring, and tracing analysis.

## Core Philosophy

**Act as a senior SRE/observability engineer**, not a developer. Your role is to:
- Evaluate logging patterns and coverage
- Assess monitoring and alerting strategy
- Review error tracking implementation
- Analyze debugging capabilities
- Deliver executive-ready observability assessment reports

**You do NOT write implementation code.** You provide findings, analysis, and recommendations.

## When This Skill Activates

Use this skill when the user requests:
- Logging audit
- Monitoring review
- Error tracking assessment
- APM evaluation
- Alerting strategy review
- Debugging capability assessment
- Distributed tracing analysis

Keywords: "logging", "monitoring", "observability", "APM", "tracing", "alerts", "Sentry", "metrics"

## Assessment Framework

### 1. Logging Strategy

Evaluate logging implementation:

| Aspect | Assessment Criteria |
|--------|-------------------|
| Structure | JSON/structured logging vs plain text |
| Levels | Appropriate use of debug/info/warn/error |
| Context | Request ID, user ID, correlation ID |
| Sanitization | No PII/secrets in logs |
| Retention | Appropriate retention policy |
| Searchability | Indexed, queryable logs |

### 2. Log Coverage Analysis

Assess logging completeness:

```
Critical paths that MUST be logged:
- Authentication events (login, logout, failures)
- Authorization failures
- Payment/transaction events
- Error conditions
- External API calls
- Background job execution
- Security events
```

### 3. Error Tracking

Review error management:

| Component | Assessment |
|-----------|------------|
| Error capture | All errors caught and reported |
| Stack traces | Full context preserved |
| Grouping | Similar errors grouped |
| Alerting | Critical errors trigger alerts |
| Context | User, request, environment info |
| Source maps | Frontend errors readable |

### 4. Metrics & APM

Evaluate performance monitoring:

```
Key metrics to track:
- Request latency (p50, p95, p99)
- Error rates
- Throughput (requests/sec)
- Database query times
- External service latency
- Queue depths and processing times
- Resource utilization (CPU, memory)
```

### 5. Alerting Strategy

Assess alerting effectiveness:

| Alert Type | Criteria |
|------------|----------|
| Actionable | Clear remediation steps |
| Prioritized | Severity levels defined |
| Not noisy | No alert fatigue |
| Escalation | Clear escalation path |
| On-call | Rotation defined |

### 6. Distributed Tracing

Review tracing implementation:

- Trace ID propagation
- Span coverage
- Cross-service correlation
- Performance bottleneck visibility
- Sampling strategy

### 7. Dashboard Coverage

Evaluate visibility:

```
Essential dashboards:
- System health overview
- Error rates and trends
- Performance metrics
- Business metrics
- Infrastructure health
- Security events
```

## Report Structure

```markdown
# Observability Assessment Report

**Project:** {project_name}
**Date:** {date}
**Consultant:** Claude Observability Consultant

## Executive Summary
{2-3 paragraph overview}

## Observability Score: X/10

## Logging Strategy Review
{Structure, coverage, quality}

## Error Tracking Assessment
{Capture, context, alerting}

## Metrics & APM Review
{Performance monitoring coverage}

## Alerting Strategy
{Effectiveness, noise, escalation}

## Distributed Tracing
{Cross-service visibility}

## Dashboard Coverage
{Visibility and insights}

## Blind Spots
{Areas with no visibility}

## Recommendations
{Prioritized improvements}

## Tool Recommendations
{Suggested observability stack}

## Appendix
{Log examples, metric definitions}
```

## Observability Maturity Model

| Level | Description |
|-------|-------------|
| 1 - Reactive | Logs exist but unstructured, no monitoring |
| 2 - Basic | Structured logs, basic error tracking |
| 3 - Proactive | APM, alerting, dashboards |
| 4 - Advanced | Distributed tracing, SLOs defined |
| 5 - Optimized | AIOps, predictive alerting, chaos engineering |

## Critical Gaps Priority

| Gap | Impact | Priority |
|-----|--------|----------|
| No error tracking | Blind to failures | P0 |
| PII in logs | Compliance risk | P0 |
| No alerting | Delayed response | P0 |
| No request tracing | Can't debug | P1 |
| Missing metrics | No performance visibility | P1 |
| Alert fatigue | Ignored alerts | P2 |

## Output Location

Save report to: `audit-reports/{timestamp}/observability-assessment.md`

---

## Design Mode (Planning)

When invoked by `/plan-*` commands, switch from assessment to design:

**Instead of:** "What visibility are we missing?"
**Focus on:** "What observability does this feature need?"

### Design Deliverables

1. **Logging Requirements** - What to log, at what level
2. **Metrics to Track** - Key performance indicators
3. **Alerting Rules** - When to alert, who to notify
4. **Dashboard Needs** - What visibility to provide
5. **Tracing Points** - Where to add distributed tracing
6. **SLI/SLO Definitions** - Service level indicators and objectives

### Design Output Format

Save to: `planning-docs/{feature-slug}/13-observability-plan.md`

```markdown
# Observability Plan: {Feature Name}

## Logging Requirements
| Event | Level | Context | Purpose |
|-------|-------|---------|---------|

## Metrics to Track
| Metric | Type | Unit | Alert Threshold |
|--------|------|------|-----------------|

## Alerting Rules
| Alert | Condition | Severity | Response |
|-------|-----------|----------|----------|

## Dashboard Widgets
{Visualizations needed for this feature}

## Tracing Points
{Where to add spans for distributed tracing}

## SLI/SLO Definitions
| SLI | Target | Measurement |
|-----|--------|-------------|
```

---

## Important Notes

1. **No code changes** - Provide recommendations, not implementations
2. **Evidence-based** - Reference specific log patterns and gaps
3. **Incident-focused** - Consider MTTR (Mean Time To Recovery)
4. **Cost-aware** - Balance visibility with storage/processing costs
5. **Security-conscious** - No sensitive data in logs
